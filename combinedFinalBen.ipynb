{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The titanic machine learning competition...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "from collections import Counter\n",
    "\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as pl\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "# Tuning\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import and read data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The data provided is split into two groups:\n",
    "1) The training set (train.csv)\n",
    "2) The testing set (test.csv)\n",
    "\n",
    "The training set includes a survival column which indicates whether or not the passenger survived. This data set is used to create the machine learning model.\n",
    "The testing set is used to determine how well the model (generated from the training data set) performs on new unseen data. The testing data set does not provide the passengers' survival status. The model generated predicts the passengers' survival status.\n",
    "\n",
    "The table below provides all the relevant information about the columns in the data sets:\n",
    "\n",
    "| Column Name          | Description                                                | Key                    |\n",
    "| ---------------------| ---------------------------------------------------------- | ---------------------- |\n",
    "| __PassengerId__      | Passenger Identity                                         |                        | \n",
    "| __Survived__         | Passenger survival status                                  | 0 = No, 1 = Yes        | \n",
    "| __Pclass__           | Ticket class, a representation of socio-economic status (SES)| 1 = 1st class, 2 = 2nd class, 3 = 3rd class | \n",
    "| __Name__             | Passenger's name                                           |                        | \n",
    "| __Sex__              | Passenger's sex                                            |                        |\n",
    "| __Age__              | Passengers age (in years)                                  |                        |\n",
    "| __SibSp__            | Number of sibling and/or spouse travelling with passenger  |                        |\n",
    "| __Parch__            | Number of parent and/or children travelling with passenger |                        |\n",
    "| __Ticket__           | Ticket number                                              |                        |\n",
    "| __Fare__             | Price of the ticket                                        |                        |\n",
    "| __Cabin__            | Cabin number                                               |                        |\n",
    "| __Embarked__         | Point of embarkation                                       | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "More information can be found under the [data](https://www.kaggle.com/c/titanic/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Exploratory data analysis is used to gain insight on the data provided. This is achieved by using visualisation tools such as graphs and tables. It will allow us to understand the data and derive preliminary conclusions. Furthermore, it will summerise important trends, characteristics, and abnormalities in the dataset which will ultimately aid in training the model.\n",
    "\n",
    "The following is explored and analysed:\n",
    "- Data Types\n",
    "- The shape of the data\n",
    "- Missing values in the data\n",
    "- Statistics derived from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data types,data shapes, missing data and summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1.Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-null count and data types of the training\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='pink'>Observation:</font>  The training-set has 891 rows and 11 features including the __target variable (survived).__ 2 of the features are floats, 5 are integers and 5 are objects. When training, the model requires the data to all be in the form of numbers, therefore these columns will be converted later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2.Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"The shape of the training data set: \", train.shape)\n",
    "print(\"The shape of the testing data set: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The testing data set has one column less column than the training data set (the Survived column). As discussed above in section 3, survived is our response/target variable and will therefore be determined from the model derived from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3.Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Determine what percentage of data is missing values in each column of the training dataset\n",
    "totalNumberOfDataPoints = train.isnull().sum().sort_values(ascending=False)\n",
    "percentMissing = train.isnull().sum()/train.isnull().count()*100\n",
    "percentMissingRounded = (round(percentMissing, 1)).sort_values(ascending=False)\n",
    "missingData = pd.concat([totalNumberOfDataPoints, percentMissingRounded], axis=1, keys=['Total missing', '%'])\n",
    "missingData.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Determine what percentage of data is missing values in each column of the testing dataset\n",
    "totalNumberOfDataPoints = test.isnull().sum().sort_values(ascending=False)\n",
    "percentMissing = test.isnull().sum()/test.isnull().count()*100\n",
    "percentMissingRounded = (round(percentMissing, 1)).sort_values(ascending=False)\n",
    "missingData = pd.concat([totalNumberOfDataPoints, percentMissingRounded], axis=1, keys=['Total missing', '%'])\n",
    "missingData.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> From the two tables above it can be seen that the training set has missing values in the Cabin, Age and Embarked columns. The testing dataset has missing values in the Cabin, Age and Fare columns. \n",
    "For the training dataset, the Embarked column only contains two missing values which can be easily dropped or filled. The Age column on the other hand has 177 missing values. We therefore, cannot drop the rows which have missing values in the age column as this will eliminate 20% of the training data. Therefore, these values need to be filled in. The approach taken to fill in the missing values is discussed below in section 5.2. Since the Cabin column is missing 77% of data points, we have decided to drop this column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4.Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summary of the statistics for the training data set \n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The table above gives an overview of the central tendencies of the numeric data in the testing dataset. <br /> <font color='pink'>Observations:</font> \n",
    "- 38% of people in the training dataset survived the Titanic \n",
    "- The passenger age ranges from 0.4 to 80 years old.\n",
    "- There is an outlier in the Fare column because of the differences between the 75th percentile, standard deviation, and the max value (512). We will thus determine how to deal with this outlier by either dropping its corresponding row or filling the outlier with an appropriate value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For feature analysis the training dataset will be split into two categories:\n",
    "1) Categorical variables\n",
    "2) Numerical variables\n",
    "\n",
    "Categorical variables have values belonging to one of two or more categories. Numerical variables have a continuous distribution.\n",
    "Identifying which variables are categorical and which variables are numerical will hel structure the data analysis properly. For example it makes no sense to determine the average of a categorical variable such as sex or class. Furthermore, sex, class and embarked have no intrinsic ordering to its value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this data set the categorical variables are:\n",
    "1) Sex\n",
    "2) Pclass \n",
    "3) Embarked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.1.Categorical variable: Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of the sex column\n",
    "train['Sex'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> There are 263 more male passengers than female passengers in the training dataset. Therefore there it is assumend that the test dataset will have a similiar distribution of sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mean of survival according to sex\n",
    "train[['Sex', 'Survived']].groupby('Sex', as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# visualisation for the probability of survival according to sex\n",
    "sns.barplot(x = 'Sex', y ='Survived', data = train)\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.title('Survival Probability by Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> Female passengers are more likely to survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.2.Categorical variable: Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Value counts of the Pclass column in the training dataset\n",
    "\n",
    "train['Pclass'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mean of survival by passenger class in the training dataset\n",
    "\n",
    "train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#Pclass distributions for survived and not survived\n",
    "ax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Pclass'],shade=True,color='r',label='Not Survived')\n",
    "ax.legend()\n",
    "ax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'],shade=True,color='b',label='Survived')\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25)\n",
    "labels = ['First', 'Second', 'Third']\n",
    "plt.xticks(sorted(train.Pclass.unique()),labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.barplot(x = 'Pclass', y ='Survived', data = train)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by Passenger Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The probability of survival decreases with a decrease in passenger class. It can therefore be assumed that first class passengers were prioritised during the evacuation. Evidently, from the two graphs above, Pclass plays an important role in determining whether a passenger did or did not survive. According to the training dataset, 63% of the 1st class passengers survived, 48% of the 2nd class passengers survived and only 24% of the 3rd class passengers survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.3.Categorical variables combined: Sex and Plass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Survival by gender and passenger class\n",
    "sns.factorplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = train, kind = 'bar').despine(left = True)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by Sex and Passenger Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font>The graph above indicates that in every class, females where always more likely to survive. It can also be seen that males in the first class were more likely to survive than in any other class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.4.Categorical variable: Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Value counts of the Embarked column \n",
    "#NAN is the missing values in Embarked\n",
    "train['Embarked'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of survival by point of embarkation\n",
    "train[['Embarked', 'Survived']].groupby(['Embarked'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation for the probability of survival according to point of embarkation\n",
    "sns.barplot(x = 'Embarked', y ='Survived', data = train)\n",
    "plt.ylabel('Probability of Survival')\n",
    "plt.title('Survival Probability by Point of Embarkation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The probability of survival is highest for location C and lowest for location S.\n",
    "Perhaps first class passengers embarked from location C and therefore because first class passengers had a higher chance of survival, location c also has the highest chance of survival. As an alternative perhaps third class passengers embarked from location S and because third class passengers had the lowest chance of survival , location S also has the lowest survival probability. This hypothesis is tested in section 4.2.1.5 below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.5.Categorical variable combined: Embarked and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Visualisation for the relationship between class and embark \n",
    "sns.factorplot('Pclass', col = 'Embarked', data = train, kind = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The hypothesis discussed in section 4.1.2.4 appears to be correct.  Location S has majority of the third class passengers and the majority of passengers embarking from location C are first class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia\n",
    "In this dataset, the numerical variables are:\n",
    "1) SibSp\n",
    "2) Parch\n",
    "3) Age\n",
    "4) Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.1.Numerical variables correlation with survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap of numerical variables\n",
    "df_num = train[['Survived','Age','SibSp','Parch','Fare']]\n",
    "sns.heatmap(df_num.corr(), annot=True,cmap=\"RdBu\")\n",
    "plt.title(\"Correlations Among Numeric Features\", fontsize = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The heatmap displayed above shows that Parch and SiSp often travel together.Therefore it will useful to create a isAlone and a family size feature.\n",
    "we also see that Fare has a pretty large positive correlation to survival, thus it may be an important metric in training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.2.Numerical variable: SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of the SibSp column \n",
    "train['SibSp'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of survival by SibSp\n",
    "train[['SibSp', 'Survived']].groupby('SibSp', as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation for probability of survival according to SiSP \n",
    "sns.barplot(x = 'SibSp', y ='Survived', data = train)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by SibSp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.3.Numerical variable: Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of the Parch column \n",
    "train['Parch'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of survival by Parch\n",
    "train[['Parch', 'Survived']].groupby('Parch', as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation for probability of survival according to Parch\n",
    "sns.barplot(x = 'Parch', y ='Survived', data = train)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by Parch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.4.Numerical variable: Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passenger age distribution\n",
    "sns.distplot(train['Age'], label = 'Skewness: %.2f'%(train['Age'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Passenger Age Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution by survival\n",
    "sns.FacetGrid(train, col = 'Survived').map(sns.distplot, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(train['Age'][train['Survived'] == 0], label = 'Did not survive')\n",
    "sns.kdeplot(train['Age'][train['Survived'] == 1], label = 'Survived')\n",
    "plt.xlabel('Age')\n",
    "plt.legend()\n",
    "plt.title('Passenger Age Distribution by Survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.5.Numerical variable: Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passenger fare distribution\n",
    "sns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylabel('Passenger Fare Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The majority of passengers paid low fares where there are few people who paid very large fares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.6.Detect outliers in numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are points in the dataset that don't conform with majority of the data (they are extreme values). Outliers need to be addressed as they tend to skew data and can cause inaccurate model predictions. The Tukey method is used to detect these outliers. Outliers can only be determined for numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to predict outliers\n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\"\n",
    "    This function loops through the list of features and detects outliers in each feature. A data point is considered to be \n",
    "    an outlier if it is less than Q1-1.5*IQR or if it is greater than Q3+1.5*IQR. Once the outliers have been determined for \n",
    "    a feature, their indices will be stored in a list and then the loop will proceed to the next feature. This process repeats\n",
    "    until the last feature is complete. Finally, using the list with the indices of the outliers, the frequency of outliers is\n",
    "    determined and if the frequency is greater than n then the list fill be returned.    \n",
    "    \"\"\"\n",
    "    outlierIndices = [] \n",
    "    for col in features: \n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlierStep = 1.5 * IQR \n",
    "        outlierList = df[(df[col] < Q1 - outlierStep) | (df[col] > Q3 + outlierStep)].index\n",
    "        outlierIndices.extend(outlierList) \n",
    "    outlierIndices = Counter(outlierIndices)\n",
    "    multipleOutliers = list(key for key, value in outlierIndices.items() if value > n) \n",
    "    return multipleOutliers\n",
    "outliers_to_drop_test = detect_outliers(test, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\n",
    "outliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\n",
    "print(\"The indices where outliers occur are {}: \".format(len(outliers_to_drop)), outliers_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers in numerical variables\n",
    "#Visualise the 10 rows identified above as rows containing outliers\n",
    "train.loc[outliers_to_drop, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Correlation between categorical and numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3.1.All variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare','Pclass']].corr(), annot = True, fmt = '.2f', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font>  Fare appears to have a high correlation with survival and Pclass has a high ngeative correlation with survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data preprocessing\n",
    "\n",
    "Now that we know what features are correlated and some other important factors, we can get the dataset into a form to be modelled and trained. This includes:\n",
    "- Dealing with ouliers\n",
    "- Drop and fill missing values\n",
    "- Data transformation \n",
    "- Feature engineering\n",
    "- Feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers \n",
    "\n",
    "print(\"Train Set Before: {} rows\".format(len(train)))\n",
    "#train = train.drop(outliers_to_drop, axis = 0).reset_index(drop = True)\n",
    "print(\"Train Set After: {} rows\".format(len(train)))\n",
    "print(\"Test Set Before: {} rows\".format(len(test)))\n",
    "# test = test.drop(outliers_to_drop_test, axis = 0).reset_index(drop = True)\n",
    "print(\"Test Set After: {} rows\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font>  Dropping outliers does not seem to improve scores later on, therefore only drop for specific Machine Learning Algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Drop and fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ticket and cabin features from training and test set as they are unique or missing many values\n",
    "train = train.drop(['Ticket', 'Cabin'], axis = 1)\n",
    "test = test.drop(['Ticket', 'Cabin'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill missing value in Embarked with mode as only 3 values\n",
    "mode = train['Embarked'].dropna().mode()[0]\n",
    "train['Embarked'].fillna(mode, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing value for Fare \n",
    "median = test['Fare'].dropna().median()\n",
    "test['Fare'].fillna(median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check where indeces of missing ages are\n",
    "age_nan_indices_train = list(train[train['Age'].isnull()].index)\n",
    "len(age_nan_indices_train)\n",
    "age_nan_indices_test = list(test[test['Age'].isnull()].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age is negatively correlated with SibSp, Parch and Pclass as shown in section 4. Loop through each of the rows which have the same corresponding values and fill the missing age with their median. Otherwise fill with the Age median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = train['Age'].median()\n",
    "\n",
    "for index in age_nan_indices_train:\n",
    "    predict_age = train['Age'][(train['SibSp'] == train.iloc[index]['SibSp']) \n",
    "                                 & (train['Parch'] == train.iloc[index]['Parch'])\n",
    "                                 & (train['Pclass'] == train.iloc[index][\"Pclass\"])].median()\n",
    "    if np.isnan(predict_age):\n",
    "        train['Age'].iloc[index] = median_age\n",
    "    else:\n",
    "        train['Age'].iloc[index] = predict_age\n",
    "combine = pd.concat([train, test], axis = 0).reset_index(drop = True)\n",
    "median_age = combine['Age'].median()\n",
    "for index in age_nan_indices_test:\n",
    "    #use larger sample to fill test data \n",
    "    test['Age'].iloc[index] = median_age  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are no more missing ages \n",
    "print(train['Age'].isnull().sum())\n",
    "test['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Data transformation\n",
    "\n",
    "Apply a log transformation to fair as it has a high right-skewnewss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  fare distribution\n",
    "\n",
    "sns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Passenger Fare Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to reduce skewness in fare, apply log transformation \n",
    "train['Fare'] = train['Fare'].map(lambda x: np.log(x) if x > 0 else 0)\n",
    "test['Fare'] = test['Fare'].map(lambda x: np.log(x) if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After log transformation\n",
    "\n",
    "sns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Fare Distribution After Log Transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Feature engineering\n",
    "\n",
    "We create new features from existing features to obtain an improved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title from name column\n",
    "train['Title'] = [name.split(',')[1].split('.')[0].strip() for name in train['Name']]\n",
    "train[['Name', 'Title']].head()\n",
    "test['Title'] = [name.split(',')[1].split('.')[0].strip() for name in test['Name']]\n",
    "test[['Name', 'Title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of Title\n",
    "train['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the testing titles\n",
    "test['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify Title as there are several unique titles that do not necessarily have a trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'] = train['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Lady', 'Jonkheer', 'Don', 'Capt', 'the Countess',\n",
    "                                             'Sir'], 'Rare')\n",
    "train['Title'] = train['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "train['Title'] = train['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "test['Title'] = test['Title'].replace(['Dr', 'Rev', 'Col',  'Capt', 'Dona'], 'Rare')\n",
    "test['Title'] = test['Title'].replace(['Ms'], 'Miss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the name column as title has been extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('Name', axis = 1)\n",
    "train.head()\n",
    "\n",
    "test = test.drop('Name', axis = 1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['Title', 'Survived']].groupby(['Title'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> Woman and young males had a high chance of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 byThemselves\n",
    "\n",
    "Simplify and summarize data to show if passenegers were alone, as opposed to information about both siblings and parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "train[['SibSp', 'Parch', 'FamilySize']].head()\n",
    "\n",
    "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
    "test[['SibSp', 'Parch', 'FamilySize']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create byThemselves feature as familySize may have more information than we need, leading to overfitting\n",
    "\n",
    "train['byThemselves'] = 0\n",
    "train.loc[train['FamilySize'] == 1, 'byThemselves'] = 1\n",
    "\n",
    "test['byThemselves'] = 0\n",
    "test.loc[test['FamilySize'] == 1, 'byThemselves'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop SibSp, Parch and FamilySize as this is contained in byThemselves\n",
    "\n",
    "train = train.drop(['SibSp', 'Parch','FamilySize'], axis = 1)\n",
    "test = test.drop(['SibSp', 'Parch','FamilySize'], axis = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 Age*Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convert Age into an ordinal variable. Group Ages into 4 age bands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['AgeBand'] = pd.cut(train['Age'], 4)\n",
    "test['AgeBand'] = pd.cut(test['Age'], 4)\n",
    "print(train['AgeBand'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Age'] <= 20.315, 'Age'] = 0\n",
    "train.loc[(train['Age'] > 20.315) & (train['Age'] <= 40.21), 'Age'] = 1\n",
    "train.loc[(train['Age'] > 40.21) & (train['Age'] <= 60.105), 'Age'] = 2\n",
    "train.loc[train['Age'] > 60.105,'Age'] = 3\n",
    "\n",
    "test.loc[test['Age'] <= 20.315, 'Age'] = 0\n",
    "test.loc[(test['Age'] > 20.315) & (test['Age'] <= 40.21), 'Age'] = 1\n",
    "test.loc[(test['Age'] > 40.21) & (test['Age'] <= 60.105), 'Age'] = 2\n",
    "test.loc[test['Age'] > 60.105,'Age'] = 3\n",
    "\n",
    "# Drop age band feature\n",
    "train = train.drop('AgeBand', axis = 1)\n",
    "test = test.drop('AgeBand', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert ordinal Age into integer\n",
    "train['Age'] = train['Age'].astype('int')\n",
    "test['Age'] = test['Age'].astype('int')\n",
    "train['Age'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Age*Class\n",
    "\n",
    "train['Age*Class'] = train['Age'] * train['Pclass']\n",
    "test['Age*Class'] = test['Age'] * test['Pclass']\n",
    "train[['Age', 'Pclass', 'Age*Class']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin Fare \n",
    "train['FareBand'] = pd.qcut(train['Fare'], 4)\n",
    "test['FareBand'] = pd.qcut(test['Fare'], 4)\n",
    "train['FareBand'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinal encoding, simliar to age\n",
    "train.loc[train['Fare'] <= 2.066, 'Fare'] = 0\n",
    "train.loc[(train['Fare'] > 2.066) & (train['Fare'] <= 2.671), 'Fare'] = 1\n",
    "train.loc[(train['Fare'] > 2.671) & (train['Fare'] <= 3.418), 'Fare'] = 2\n",
    "train.loc[train['Fare'] > 3.418, 'Fare'] = 3\n",
    "\n",
    "test.loc[test['Fare'] <= 2.066, 'Fare'] = 0\n",
    "test.loc[(test['Fare'] > 2.066) & (test['Fare'] <= 2.671), 'Fare'] = 1\n",
    "test.loc[(test['Fare'] > 2.671) & (test['Fare'] <= 3.418), 'Fare'] = 2\n",
    "test.loc[test['Fare'] > 3.418, 'Fare'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['FareBand'], axis = 1)\n",
    "test = test.drop(['FareBand'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Fare into integer\n",
    "train['Fare'] = train['Fare'].astype('int')\n",
    "test['Fare'] = test['Fare'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Feature encoding \n",
    "\n",
    "Variables must be numeric to use for machine learning. Age and Fare were done when Binning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label = LabelEncoder() \n",
    "train['Embarked'] = label.fit_transform(train['Embarked'])\n",
    "test['Embarked'] = label.fit_transform(test['Embarked'])\n",
    "train['Title'] = label.fit_transform(train['Title'])\n",
    "test['Title'] = label.fit_transform(test['Title'])\n",
    "train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\n",
    "test['Sex'] = test['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.drop('PassengerId', axis = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Survived'] = train['Survived'].astype('int')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train[['Survived', 'Pclass', 'Sex', 'Age', 'Fare','Embarked','Title','byThemselves','Age*Class']].corr(), annot = True, fmt = '.2f', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After data preprocessing, we can see how all of the new and remaining features are correlated in the figure above. Clearly sex has the greatest correlation with survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling\n",
    "\n",
    "For the modelling part of this project we will utilise the Scikit-learn library for machine learning. \n",
    "\n",
    " As discussed above, this is a classfication problem, so consequently will use classfication models for our training.  We have chosen to investigate the performance of the following classifiers as these are commonly used classifiers:\n",
    "\n",
    "- Logistic regression\n",
    "- Multi Layer Perceptron\n",
    "- K-nearest neighbours\n",
    "- Gaussian naive bayes\n",
    "- Linear SVC\n",
    "- Stochastic gradient descent\n",
    "- Random forest\n",
    "- Support vector machines\n",
    "\n",
    "In this section , we will fit the models to the training data set and evaluate the models' prediction accuracy. Further on we  will implement feature tuning and hyperparameter tuning to further boost the performance of the the following models:\n",
    "\n",
    "- Logistic regression\n",
    "- Multi Layer Perceptron\n",
    "- K-nearest neighbours\n",
    "- Random Forest \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('Survived', axis = 1)\n",
    "Y_train = train['Survived']\n",
    "X_test = test.drop('PassengerId', axis = 1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 K-fold cross validation\n",
    "\n",
    "It is important to not get too carried away with models with impressive training accuracy as what we should focus on instead is the model's ability to predict out-of-samples data, in other words, data our model has not seen before.\n",
    "\n",
    "This is where k-fold cross validation comes in. K-fold cross validation is a technique whereby a subset of our training set is kept aside and will act as holdout set for testing purposes. Here is a great [video](https://www.youtube.com/watch?v=fSytzGwwBVw) explaining the concept in more detail. \n",
    "\n",
    "We compute the accuracy and cross-entropy log-loss for each algorithm to determine its performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list which contains classifiers \n",
    "classifiers = [ MLPClassifier(),\n",
    "                LogisticRegression(), \n",
    "                GaussianNB(),\n",
    "                KNeighborsClassifier(n_neighbors = 5), \n",
    "                SVC(),  \n",
    "                SGDClassifier(), \n",
    "                LinearSVC(),\n",
    "                RandomForestClassifier(),\n",
    "               ]\n",
    "\n",
    "if len(classifiers) is not 8: \n",
    "    print(\"error\")\n",
    "\n",
    "cross_val_results = []\n",
    "cross_val_losses = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    cross_val_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy', cv = 5))\n",
    "    cross_val_losses.append(cross_val_score(classifier, X_train, Y_train, scoring = 'neg_log_loss', cv = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation of cross validation results for each classifier  \n",
    "\n",
    "cross_val_mean = []\n",
    "cross_val_std = []\n",
    "cross_val_loss_mean =[]\n",
    "cross_val_loss_std = []\n",
    "\n",
    "for cross_val_result in cross_val_results:\n",
    "    cross_val_mean.append(cross_val_result.mean())\n",
    "    cross_val_std.append(cross_val_result.std())\n",
    "\n",
    "for cross_val_loss in cross_val_losses:\n",
    "    cross_val_loss_mean.append(cross_val_loss.mean())\n",
    "    cross_val_loss_std.append(cross_val_loss.std())    \n",
    "\n",
    "    \n",
    "    \n",
    "#putting our cross val mean and standard deviation for each algorithm into a data frame\n",
    "cross_val_res = pd.DataFrame({'Cross Validation Mean': cross_val_mean, 'Cross Validation Std': cross_val_std, 'Algorithm': [ 'Multilayer perceptron','Logistic Regression','Naive Bayes', 'K Nearest Neighbours', 'Support Vector Machines', 'Stochastic Gradient Decent', 'Linear SVC', 'Random Forest']})\n",
    "cross_val_los = pd.DataFrame({'Cross Validation Mean': cross_val_loss_mean, 'Cross Validation Std': cross_val_loss_std, 'Algorithm': [ 'Multilayer perceptron','Logistic Regression','Naive Bayes', 'K Nearest Neighbours', 'Support Vector Machines', 'Stochastic Gradient Decent', 'Linear SVC', 'Random Forest']})\n",
    "print('Table Showing Cross Validation Mean Accuracy')\n",
    "print(cross_val_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True))\n",
    "print('\\nTable Showing Cross Validation Mean Loss')\n",
    "print(cross_val_los.sort_values(by = 'Cross Validation Mean', ascending = True, ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> Support Vector Machines has the greatest accuracy followed by Random Forest and MLP. However, MLP has the least observable loss and would appear to be the 'best' at first glance. \n",
    "\n",
    "Nonetheless, 4 algorithms will be tuned below and the scores observed once more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('Cross Validation Mean', 'Algorithm', data = cross_val_res, order = cross_val_res.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cross_val_std})\n",
    "plt.ylabel('Algorithm')\n",
    "plt.title('Cross Validation Scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('Cross Validation Mean', 'Algorithm', data = cross_val_los, order = cross_val_los.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cross_val_loss_std})\n",
    "plt.ylabel('Algorithm')\n",
    "plt.title('Cross Validation Losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Feature Tuning\n",
    "\n",
    "The best remaining features can be found for each algorithm and the rest can be dropped for each respective algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of correct classifications (survived or did not survive) is proportional to the \"accuracy\" scoring\n",
    "RFECV1 = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\n",
    "RFECV1.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Optimal number of features: %d\" % RFECV1.n_features_)\n",
    "print('Selected features: %s' % list(X_train.columns[RFECV1.support_]))\n",
    "\n",
    "# Plot the number of features VS. the CV scores\n",
    "plt.xlabel(\"Number of features that are selected\")\n",
    "plt.ylabel(\"Cross validation score \")\n",
    "plt.plot(range(1, len(RFECV1.grid_scores_) + 1), RFECV1.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LR = X_train.drop(['byThemselves', 'Fare'], axis =1).copy()\n",
    "X_test_LR = X_test.drop(['byThemselves', 'Fare'], axis =1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFECV2 = RFECV(estimator=RandomForestClassifier(), step=1, cv=10, scoring='accuracy')\n",
    "RFECV2.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Optimal number of features: %d\" % RFECV2.n_features_)\n",
    "print('Selected features: %s' % list(X_train.columns[RFECV2.support_]))\n",
    "\n",
    "# Plot the number of features VS. the CV scores\n",
    "plt.xlabel(\"Number of features that are selected\")\n",
    "plt.ylabel(\"Cross validation score \")\n",
    "plt.plot(range(1, len(RFECV2.grid_scores_) + 1), RFECV2.grid_scores_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on the above, for Random Forest we should drop 3 columns\n",
    "X_train_RF = X_train.drop(['Age'], axis =1).copy()\n",
    "X_test_RF = X_test.drop(['Age'], axis =1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP and KNN have intrinsic feature importance selection and so do not need to have it manually performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Hyperparameter Tuning\n",
    "Hyperparameter tuning is the process of tuning the parameters of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.1 Hyperparameter tuning for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=50)\n",
    "param_grid = {'alpha': [1,1e-3,1e-5],  \n",
    "              'hidden_layer_sizes': [(50,50,50),(3,), (50,190,3), (64,64,64) ],\n",
    "              'activation': ['tanh','relu','logistic'],  \n",
    "              'solver': ['sgd','adam'],\n",
    "              'learning_rate': ['constant','adaptive']\n",
    "}\n",
    "grid = GridSearchCV(mlp, param_grid,n_jobs=-1, cv=5) \n",
    "grid.fit(X_train, Y_train)\n",
    "MLP_tuned= grid.best_estimator_\n",
    "MLP_trained_tuned=MLP_tuned.fit(X_train, Y_train)\n",
    "print(\"Best parameters: \", grid.best_params_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The multilayer perceptron(MLP) is an Artificial neural network with one or more  hidden layers. The MLP has the advantage over the Perceptron algorithm in that it is able to handle non linear mapping of inputs to outputs. The MLP is a feedforward algorithm that performs a weighted summation of the inputs and their respective weights. This summation is then fed into a activation function such as ReLU to determine the output. The output of each of this nodes is then fed into the next hidden layers and then to the output. The MLP also has back propagation which allows it to iteratively adjust the weights in the network to reduce the cost function. The MLP was chosen for hyperparameter tuning as it has one of the highest accuracy scores and one of the lowest standard deviations (Bento, 2021).\n",
    "\n",
    "\n",
    "Using the GridSearchCV method with the following parameters: a grid of hyperparameters and a K-fold cross validation value. The parameter grid includes the alpha value used for regularization(which reduces overfitting), values for how many nodes in each of the 3 hidden layers , the activation function , the solver used for backpropagation and the learning rate. The gridSearchCV then finds the parameters which result in the highest accurcay (1.17. Neural network models (supervised), n.d.).\n",
    "\n",
    "Bento, C., 2021. Multilayer Perceptron Explained with a Real-Life Example and Python Code: Sentiment Analysis. [online] Medium. Available at: <https://towardsdatascience.com/multilayer-perceptron-explained-with-a-real-life-example-and-python-code-sentiment-analysis-cb408ee93141> [Accessed 14 June 2022].\n",
    "\n",
    "scikit-learn. n.d. 1.17. Neural network models (supervised). [online] Available at: <https://scikit-learn.org/stable/modules/neural_networks_supervised.html> [Accessed 14 June 2022].\n",
    "\n",
    "(Aiden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.2 Hyperparameter tuning for KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "grid = GridSearchCV(KNeighborsClassifier(), hyperparameters, cv=5,n_jobs=-1 )\n",
    "grid.fit(X_train, Y_train)\n",
    "KNN_tuned = grid.best_estimator_\n",
    "KNN_trained_tuned=KNN_tuned.fit(X_train, Y_train)\n",
    "survival_predications2=KNN_trained_tuned.predict(X_test)\n",
    "print(\"Best parameters: \", grid.best_params_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Hyperparameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 40, 60],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [  200, 1000,1600 ,1800, 2000]}\n",
    "\n",
    "#randomzed search to decrease time frame\n",
    "grid = RandomizedSearchCV(RandomForestClassifier(), param_grid, refit = True, n_iter = 20, cv = 5, verbose=3, n_jobs =-1)\n",
    "grid.fit(X_train_RF, Y_train)\n",
    "RF_tuned = grid.best_estimator_\n",
    "RF_trained_tuned = RF_tuned.fit(X_train_RF, Y_train) #using the feature tuning sets\n",
    "print(\"Best parameters: \", grid.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest algorithm is considered a good classifying model. It is a supervised machine learning algorithm. As it was one of the selected algorithms that produced one of the highest cross-validation accuracy scores and one of the lowest cross-validation loss scores, it was chosen to be tuned and retrained. The random forest machine learning algorithm takes the output of multiple decision trees and ‘democratically’ chooses the final outcome. \n",
    "\n",
    "Decision trees themselves essentially ask true or false questions that lead to a final answer. In other words, each question (for example, ‘male’ or ‘female’?) is a node, and the outcome is a leaf node. The idea is to have results of subgroups that are similar to one another but are different from the other groups. The random forest works well because while some decision trees may be wrong, many others will be correct. Therefore, as a group, the decision trees can move in the right direction. By using a concept known as ‘bagging,’ where each individual decision tree can randomly sample from the training dataset, with replacement, random forest will have different trees and therefore a low correlation between them. The uncorrelated errors between the different trees means that they ‘protect each other’ from their individual errors. This generally results in a precise classifying algorithm overall. \n",
    "\n",
    "Using the RandomizedSearchCV method for time-efficiency, a grid of hyperparameters can be randomly sampled, with K-Fold cross-validation being performed with each combination of parameter values.  The hyperparameters that were searched through include: number of trees in the forest; number of data points per node before it must be split; minimum number of data points allowed in a leaf node; the maximum number of levels of each tree; and with or without replacement. Thereafter, the model is then retrained using the new-found ‘best’ hyperparameters\n",
    "\n",
    "(Ben)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.4 Hyperparameter tuning for LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': np.arange(1e-05, 3, 0.1, 1e-4, 1e-2)}\n",
    "scoring = {'Accuracy': 'accuracy',\n",
    "           'AUC': 'roc_auc', 'Log_loss': 'neg_log_loss'}\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), return_train_score=True,\n",
    "                  param_grid=param_grid, scoring=scoring, cv=10, refit='Accuracy')\n",
    "grid.fit(X_train_LR, Y_train)\n",
    "LR_tuned = grid.best_estimator_\n",
    "LR_trained_tuned=LR_tuned.fit(X_train_LR, Y_train)\n",
    "gia_output_3=LR_trained_tuned.predict(X_test_LR)\n",
    "print(\"Best parameters: \", grid.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results after tuning:\n",
    "\n",
    "The results after tuning 4 of the models, as well as the original cross-validation scores for the models that were utilised but not tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_results = []\n",
    "cross_val_losses = []\n",
    "# Train using each model, finding accuracy and loss. :\n",
    "cross_val_results.append(cross_val_score(RF_tuned, X_train_RF, Y_train, scoring = 'accuracy', cv = 5))\n",
    "cross_val_losses.append(cross_val_score(RF_tuned, X_train_RF, Y_train, scoring = 'neg_log_loss', cv = 5))\n",
    "cross_val_results.append(cross_val_score(KNN_tuned, X_train, Y_train, scoring = 'accuracy', cv = 5))\n",
    "cross_val_losses.append(cross_val_score(KNN_tuned, X_train, Y_train, scoring = 'neg_log_loss', cv = 5))\n",
    "cross_val_results.append(cross_val_score(LR_tuned, X_train_LR, Y_train, scoring = 'accuracy', cv = 5))\n",
    "cross_val_losses.append(cross_val_score(LR_tuned, X_train_LR, Y_train, scoring = 'neg_log_loss', cv = 5))\n",
    "cross_val_results.append(cross_val_score(MLP_tuned, X_train, Y_train, scoring = 'accuracy', cv = 5))\n",
    "cross_val_losses.append(cross_val_score(MLP_tuned, X_train, Y_train, scoring = 'neg_log_loss', cv = 5))\n",
    "\n",
    "remaining_classifiers = [  \n",
    "                GaussianNB(),\n",
    "                SVC(),  \n",
    "                SGDClassifier(), \n",
    "                LinearSVC(),\n",
    "               ]\n",
    "\n",
    "\n",
    "for classifier in remaining_classifiers:\n",
    "    cross_val_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy', cv = 5))\n",
    "    cross_val_losses.append(cross_val_score(classifier, X_train, Y_train, scoring = 'neg_log_loss', cv = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation of cross validation results for each classifier  \n",
    "\n",
    "cross_val_mean = []\n",
    "cross_val_std = []\n",
    "cross_val_loss_mean =[]\n",
    "cross_val_loss_std = []\n",
    "\n",
    "for cross_val_result in cross_val_results:\n",
    "    cross_val_mean.append(cross_val_result.mean())\n",
    "    cross_val_std.append(cross_val_result.std())\n",
    "\n",
    "for cross_val_loss in cross_val_losses:\n",
    "    cross_val_loss_mean.append(cross_val_loss.mean())\n",
    "    cross_val_loss_std.append(cross_val_loss.std())    \n",
    "\n",
    "    \n",
    "    \n",
    "#putting our cross val mean and standard deviation for each algorithm into a data frame\n",
    "cross_val_res = pd.DataFrame({'Cross Validation Mean': cross_val_mean, 'Cross Validation Std': cross_val_std, 'Algorithm': [ 'Random Forest', 'K Nearest Neighbours','Logistic Regression','Multilayer perceptron','Naive Bayes', 'Support Vector Machines', 'Stochastic Gradient Decent', 'Linear SVC']})\n",
    "cross_val_los = pd.DataFrame({'Cross Validation Mean': cross_val_loss_mean, 'Cross Validation Std': cross_val_loss_std, 'Algorithm':  [ 'Random Forest', 'K Nearest Neighbours','Logistic Regression','Multilayer perceptron','Naive Bayes', 'Support Vector Machines', 'Stochastic Gradient Decent', 'Linear SVC']})\n",
    "print('Table Showing Cross Validation Mean Accuracy')\n",
    "print(cross_val_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True))\n",
    "print('\\nTable Showing Cross Validation Mean Loss')\n",
    "print(cross_val_los.sort_values(by = 'Cross Validation Mean', ascending = True, ignore_index = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation \n",
    "\n",
    "The tuned algorithms show improvement in accuracy as compared to before being tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 Ensembles\n",
    "\n",
    "If cross-val scores were above 80 and the loss was above -2, assign a weighting of 2. Otherwise 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train the other models now with no validation set to be used in the ensemble\n",
    "GNBtrained=GaussianNB().fit(X_train, Y_train)\n",
    "SVCtrained=SVC().fit(X_train, Y_train)\n",
    "LSVCtrained=LinearSVC().fit(X_train, Y_train)\n",
    "SGDtrained=SGDClassifier().fit(X_train, Y_train)\n",
    "\n",
    "voting_classifier_hard = VotingClassifier(estimators = [('mlp',MLP_trained_tuned),('knn',KNN_trained_tuned),('knn2',KNN_trained_tuned),('lg',LR_trained_tuned),('rf',RF_trained_tuned),('rf2',RF_trained_tuned),('gnb',GNBtrained),('svc',SVCtrained),('svc2',SVCtrained),('lsvc',LSVCtrained),('sgd',SGDtrained)], voting = 'hard') \n",
    "voting_classifier_hard_trained=voting_classifier_hard.fit(X_train, Y_train)\n",
    "survival_predications=voting_classifier_hard_trained.predict(X_test)\n",
    "\n",
    "print('voting classifier cross validation score :',cross_val_score(voting_classifier_hard,X_train,Y_train,cv=5))\n",
    "print('voting classifier cross validation score mean :',cross_val_score(voting_classifier_hard,X_train,Y_train,cv=5).mean())\n",
    "print('voting classifier cross validation score standard deviation :',cross_val_score(voting_classifier_hard,X_train,Y_train,cv=5).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preparing data for submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our submission dataframe to have 418 rows and 2 columns, PassengerId and Survived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "\n",
    "submit = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': survival_predications})\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe is ready for submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save csv file \n",
    "submit.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Possible extensions to improve model accuracy\n",
    "\n",
    "1. Analyse ticket and cabin features\n",
    "    - Do these features help predict passenger survival?\n",
    "    - If yes, consider including them in the training set instead of dropping\n",
    "2. Come up with alternative features in feature engineering\n",
    "    - Is there any other features you can potentially create from existing features in the dataset\n",
    "3. Remove features that are less important\n",
    "    - Does removing features help reduce overfitting in the model?\n",
    "4. Ensemble modelling\n",
    "    - This is a more advanced technique whereby you combine prediction results from multiple machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://github.com/chongjason914/kaggle-titanic \n",
    "\n",
    "https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "\n",
    "https://github.com/murilogustineli/Titanic-Classification\n",
    "\n",
    "https://www.kaggle.com/code/kenjee/titanic-project-example/notebook\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c36ee19e8b3149dddcbe35ab5f92ddf427ca8ae53f8aa17469b1495a8f0d9a75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
