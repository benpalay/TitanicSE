{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno\n",
    "from collections import Counter\n",
    "\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import and read data\n",
    "\n",
    "Now import and read the 3 datasets as outlined in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia\n",
    "The data provided is split into two groups:\n",
    "1) The training set (train.csv)\n",
    "2) The testing set (test.csv)\n",
    "\n",
    "The training set includes a survival column which indicates whether or not the passenger survived. This data set is used to create the machine learning model.\n",
    "The testing set is used to determine how well the model (generated from the training data set) performs on new unseen data. The testing data set does not provide the passengers' survival status. The model generated predicts the passengers' survival status.\n",
    "\n",
    "The table below provides all the relevant information about the columns in the data sets:\n",
    "\n",
    "| Column Name          | Description                                                | Key                    |\n",
    "| ---------------------| ---------------------------------------------------------- | ---------------------- |\n",
    "| __PassengerId__      | Passenger Identity                                         |                        | \n",
    "| __Survived__         | Passenger survival status                                  | 0 = No, 1 = Yes        | \n",
    "| __Pclass__           | Ticket class, a representation of socio-economic status (SES)| 1 = 1st class, 2 = 2nd class, 3 = 3rd class | \n",
    "| __Name__             | Passenger's name                                           |                        | \n",
    "| __Sex__              | Passenger's sex                                            |                        |\n",
    "| __Age__              | Passengers age (in years)                                  |                        |\n",
    "| __SibSp__            | Number of sibling and/or spouse travelling with passenger  |                        |\n",
    "| __Parch__            | Number of parent and/or children travelling with passenger |                        |\n",
    "| __Ticket__           | Ticket number                                              |                        |\n",
    "| __Fare__             | Price of the ticket                                        |                        |\n",
    "| __Cabin__            | Cabin number                                               |                        |\n",
    "| __Embarked__         | Point of embarkation                                       | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "More information can be found under the [data](https://www.kaggle.com/c/titanic/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia\n",
    "Exploratory data analysis is used to gain insight on the data provided. This is achieved by using visualisation tools such as graphs and tables. It will allow us to understand the data and derive preliminary conclusions. Furthermore, it will summerise important trends, characteristics, and abnormalities in the dataset which will ultimately aid in training the model.\n",
    "\n",
    "The following is explored and analysed:\n",
    "- Data Types\n",
    "- The shape of the data\n",
    "- Missing values in the data\n",
    "- Statistics derived from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Data types,data shapes, missing data and summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1.Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-null count and data types of the training\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font>  The training-set has 891 rows and 11 features + the __target variable (survived).__ 2 of the features are floats, 5 are integers and 5 are objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2.Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "print(\"The shape of the training data set: \", train.shape)\n",
    "print(\"The shape of the testing data set: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font> The testing data set has one column less column than the training data set (the Survived column). As discussed above in section 3, survived is our response/target variable and will therefore be determined from the model derived from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3.Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "#Determine what percentage of data is missing values in each column of the training dataset\n",
    "totalNumberOfDataPoints = train.isnull().sum().sort_values(ascending=False)\n",
    "percentMissing = train.isnull().sum()/train.isnull().count()*100\n",
    "percentMissingRounded = (round(percentMissing, 1)).sort_values(ascending=False)\n",
    "missingData = pd.concat([totalNumberOfDataPoints, percentMissingRounded], axis=1, keys=['Total missing', '%'])\n",
    "missingData.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "#Determine what percentage of data is missing values in each column of the testing dataset\n",
    "totalNumberOfDataPoints = test.isnull().sum().sort_values(ascending=False)\n",
    "percentMissing = test.isnull().sum()/test.isnull().count()*100\n",
    "percentMissingRounded = (round(percentMissing, 1)).sort_values(ascending=False)\n",
    "missingData = pd.concat([totalNumberOfDataPoints, percentMissingRounded], axis=1, keys=['Total missing', '%'])\n",
    "missingData.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font> From the two tables above it can be seen that the training set has missing values in the Cabin, Age and Embarked columns. The testing dataset has missing values in the Cabin, Age and Fare columns. \n",
    "For the training dataset, the Embarked column only contains two missing values which can be easily dropped or filled. The Age column on the other hand has 177 missing values. We therefore, cannot drop the rows which have missing values in the age column as this will eliminate 20% of the training data. Therefore, these values need to be filled in. The approach taken to fill in the missing values is discussed below in section 5.2. Since the Cabin column is missing 77% of data points, we have decided to drop this column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4.Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Summary of the statistics for the training data set \n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia\n",
    "The table above gives an overview of the central tendencies of the numeric data in the testing dataset. <br /> <font color='pink'>Observations:</font> \n",
    "- 38% of people in the training dataset survived the Titanic \n",
    "- The passenger age ranges from 0.4 to 80 years old.\n",
    "- There is an outlier in the Fare column because of the differences between the 75th percentile, standard deviation, and the max value (512). We will thus determine how to deal with this outlier by either dropping its corresponding row or filling the outlier with an appropriate value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia\n",
    "For feature analysis the training dataset will be split into two categories:\n",
    "1) Categorical variables\n",
    "2) Numerical variables\n",
    "\n",
    "Categorical variables have values belonging to one of two or more categories. Numerical variables have a continuous distribution.\n",
    "Identifying which variables are categorical and which variables are numerical will hel structure the data analysis properly. For example it makes no sense to determine the average of a categorical variable such as sex or class. Furthermore, sex, class and embarked have no intrinsic ordering to its value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia\n",
    "In this data set the categorical variables are:\n",
    "1) Sex\n",
    "2) Pclass \n",
    "3) Embarked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.1.Categorical variable: Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Value counts of the sex column\n",
    "train['Sex'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font> There are 263 more male passengers than female passengers in the training dataset. Therefore there is a high probability that this observation will also occur in the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Mean of survival according to sex\n",
    "train[['Sex', 'Survived']].groupby('Sex', as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gia\n",
    "# visualisation for the probability of survival according to sex\n",
    "sns.barplot(x = 'Sex', y ='Survived', data = train)\n",
    "plt.ylabel('Probability of survival')\n",
    "plt.title('Survival Probability by Sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font> Female passengers are more likely to survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.2.Categorical variable: Pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Value counts of the Pclass column in the training dataset\n",
    "\n",
    "train['Pclass'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Mean of survival by passenger class in the training dataset\n",
    "\n",
    "train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia \n",
    "#Pclass distributions for survived and not survived\n",
    "ax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Pclass'],shade=True,color='r',label='Not Survived')\n",
    "ax.legend()\n",
    "ax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'],shade=True,color='b',label='Survived')\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25)\n",
    "labels = ['First', 'Second', 'Third']\n",
    "plt.xticks(sorted(train.Pclass.unique()),labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gia\n",
    "sns.barplot(x = 'Pclass', y ='Survived', data = train)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by Passenger Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The probability of survival decreases with a decrease in passenger class. It can therefore be assumed that first class passengers were prioritised during the evacuation. Evidently, from the two graphs above, Pclass plays an important role in determining whether a passenger did or did not survive. According to the training dataset, 63% of the 1st class passengers survived, 48% of the 2nd class passengers survived and only 24% of the 3rd class passengers survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.3.Categorical variables combined: Sex and Plass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Survival by gender and passenger class\n",
    "sns.factorplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = train, kind = 'bar').despine(left = True)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by Sex and Passenger Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Aiden did this\n",
    "Gia <font color='pink'>Observation:</font>The graph above indicates that in every class, females where always more likely to survive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.4.Categorical variable: Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Value counts of the Embarked column \n",
    "#NAN is the missing values in Embarked\n",
    "train['Embarked'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Mean of survival by point of embarkation\n",
    "train[['Embarked', 'Survived']].groupby(['Embarked'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "#Visualisation for the probability of survival according to point of embarkation\n",
    "sns.barplot(x = 'Embarked', y ='Survived', data = train)\n",
    "plt.ylabel('Probability of Survival')\n",
    "plt.title('Survival Probability by Point of Embarkation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font> The probability of survival is highest for location C and lowest for location S.\n",
    "Perhaps first class passengers embarked from location C and therefore because first class passengers had a higher chance of survival, location c also has the highest chance of survival. As an alternative perhaps third class passengers embarked from location S and because third class passengers had the lowest chance of survival , location S also has the lowest survival probability. This hypothesis is tested in section 4.2.1.5 below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.5.Categorical variable combined: Embarked and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Visualisation for the relationship between class and embark \n",
    "sns.factorplot('Pclass', col = 'Embarked', data = train, kind = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='pink'>Observation:</font> The hypothesis discussed in section 4.1.2.4 appears to be correct.  Location S has majority of the third class passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia\n",
    "In this dataset, the numerical variables are:\n",
    "1) SibSp\n",
    "2) Parch\n",
    "3) Age\n",
    "4) Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.1.Detect outliers in numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia Outliers are points in the dataset that don't conform with majority of the data (they are extreme values). Outliers need to be addressed as they tend to skew data and can cause inaccurate model predictions. The Tukey method is used to detect these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "#Function to predict outliers\n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\"\n",
    "    This function loops through the list of features and detects outliers in each feature. A data point is considered to be \n",
    "    an outlier if it is less than Q1-1.5*IQR or if it is greater than Q3+1.5*IQR. Once the outliers have been determined for \n",
    "    a feature, their indices will be stored in a list and then the loop will proceed to the next feature. This process repeats\n",
    "    until the last feature is complete. Finally, using the list with the indices of the outliers, the frequency of outliers is\n",
    "    determined and if the frequency is greater than n then the list fill be returned.    \n",
    "    \"\"\"\n",
    "    outlierIndices = [] \n",
    "    for col in features: \n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlierStep = 1.5 * IQR \n",
    "        outlierList = df[(df[col] < Q1 - outlierStep) | (df[col] > Q3 + outlierStep)].index\n",
    "        outlierIndices.extend(outlierList) \n",
    "    outlierIndices = Counter(outlierIndices)\n",
    "    multipleOutliers = list(key for key, value in outlierIndices.items() if value > n) \n",
    "    return multipleOutliers\n",
    "\n",
    "outliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\n",
    "print(\"The indices where outliers occur are {}: \".format(len(outliers_to_drop)), outliers_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Outliers in numerical variables\n",
    "#Visualise the 10 rows identified above as rows containing outliers\n",
    "train.loc[outliers_to_drop, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.2.Numerical variables correlation with survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "#Heatmap of numerical variables\n",
    "df_num = train[['Age','SibSp','Parch','Fare']]\n",
    "sns.heatmap(df_num.corr(), annot=True,cmap=\"RdBu\")\n",
    "plt.title(\"Correlations Among Numeric Features\", fontsize = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia<font color='pink'>Observation:</font> The heatmap displayed above shows that Parch and SiSP often travel together.Therefore it will useful to create a isAlone and a family size feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.3.Numerical variable: SibSp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Value counts of the SibSp column \n",
    "train['SibSp'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Mean of survival by SibSp\n",
    "train[['SibSp', 'Survived']].groupby('SibSp', as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "#Visualisation for probability of survival according to SiSP \n",
    "sns.barplot(x = 'SibSp', y ='Survived', data = train)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by SibSp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.4.Numerical variable: Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Value counts of the Parch column \n",
    "train['Parch'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Mean of survival by Parch\n",
    "train[['Parch', 'Survived']].groupby('Parch', as_index = False).mean().sort_values(by = 'Survived', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "#Visualisation for probability of survival according to Parch\n",
    "sns.barplot(x = 'Parch', y ='Survived', data = train)\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.title('Survival Probability by Parch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.5.Numerical variable: Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Passenger age distribution\n",
    "\n",
    "sns.distplot(train['Age'], label = 'Skewness: %.2f'%(train['Age'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Passenger Age Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gia\n",
    "# Age distribution by survival\n",
    "sns.FacetGrid(train, col = 'Survived').map(sns.distplot, 'Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(train['Age'][train['Survived'] == 0], label = 'Did not survive')\n",
    "sns.kdeplot(train['Age'][train['Survived'] == 1], label = 'Survived')\n",
    "plt.xlabel('Age')\n",
    "plt.legend()\n",
    "plt.title('Passenger Age Distribution by Survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2.6.Numerical variable: Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passenger fare distribution\n",
    "sns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.ylabel('Passenger Fare Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Correlation between categorical and numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3.1.All variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare','Pclass']].corr(), annot = True, fmt = '.2f', cmap='RdBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gia <font color='pink'>Observation:</font>  Fare appears to have a high correlation with survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data preprocessing\n",
    "\n",
    "Getting the dataset in a form to be modelled and trained. This includes:\n",
    "- Dealing with ouliers\n",
    "- Drop and fill missing values\n",
    "- Data transformation \n",
    "- Feature engineering\n",
    "- Feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outliers \n",
    "\n",
    "print(\"Train Set Before: {} rows\".format(len(train)))\n",
    "#train = train.drop(outliers_to_drop, axis = 0).reset_index(drop = True)\n",
    "print(\"Train Set After: {} rows\".format(len(train)))\n",
    "print(\"Test Set Before: {} rows\".format(len(test)))\n",
    "# test = test.drop(outliers_to_drop_test, axis = 0).reset_index(drop = True)\n",
    "print(\"Test Set After: {} rows\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Drop and fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ticket and cabin features from training and test set as they are unique or missing many values\n",
    "train = train.drop(['Ticket', 'Cabin'], axis = 1)\n",
    "test = test.drop(['Ticket', 'Cabin'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to drop both ticket and cabin for simplicity of this tutorial but if you have the time, I would recommend going through them and see if they can help improve your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill missing value in Embarked with mode as only 3 values\n",
    "mode = train['Embarked'].dropna().mode()[0]\n",
    "train['Embarked'].fillna(mode, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing value for Fare \n",
    "median = test['Fare'].dropna().median()\n",
    "test['Fare'].fillna(median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check where indeces of missing ages are\n",
    "age_nan_indices_train = list(train[train['Age'].isnull()].index)\n",
    "len(age_nan_indices_train)\n",
    "age_nan_indices_test = list(test[test['Age'].isnull()].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age is negatively correlated with SibSp, Parch and Pclass as shown in section 4. Loop through each those rows and fill the missing age with their median. Othwerise fill with the Age median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in age_nan_indices_train:\n",
    "    median_age = train['Age'].median()\n",
    "    predict_age = train['Age'][(train['SibSp'] == train.iloc[index]['SibSp']) \n",
    "                                 & (train['Parch'] == train.iloc[index]['Parch'])\n",
    "                                 & (train['Pclass'] == train.iloc[index][\"Pclass\"])].median()\n",
    "    if np.isnan(predict_age):\n",
    "        train['Age'].iloc[index] = median_age\n",
    "    else:\n",
    "        train['Age'].iloc[index] = predict_age\n",
    "combine = pd.concat([train, test], axis = 0).reset_index(drop = True)\n",
    "median_age = combine['Age'].median()\n",
    "for index in age_nan_indices_test:\n",
    "    #use larger sample to fill test data \n",
    "    test['Age'].iloc[index] = median_age  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there are no more missing ages \n",
    "print(train['Age'].isnull().sum())\n",
    "test['Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Data transformation\n",
    "\n",
    "Recall that our passenger fare column has a very high positive skewness. Therefore, we will apply a log transformation to address this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  fare distribution\n",
    "\n",
    "sns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Passenger Fare Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to reduce skewness in fare, apply log transformation \n",
    "train['Fare'] = train['Fare'].map(lambda x: np.log(x) if x > 0 else 0)\n",
    "test['Fare'] = test['Fare'].map(lambda x: np.log(x) if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After log transformation\n",
    "\n",
    "sns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Fare Distribution After Log Transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Feature engineering\n",
    "\n",
    "We create new features from existing features to obtain an improved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title from name column\n",
    "train['Title'] = [name.split(',')[1].split('.')[0].strip() for name in train['Name']]\n",
    "train[['Name', 'Title']].head()\n",
    "test['Title'] = [name.split(',')[1].split('.')[0].strip() for name in test['Name']]\n",
    "test[['Name', 'Title']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of Title\n",
    "train['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the testing titles\n",
    "test['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify Title as there are several unique itles that do not necessarily have a trend\n",
    "\n",
    "train['Title'] = train['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Lady', 'Jonkheer', 'Don', 'Capt', 'the Countess',\n",
    "                                             'Sir'], 'Rare')\n",
    "train['Title'] = train['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "train['Title'] = train['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "test['Title'] = test['Title'].replace(['Dr', 'Rev', 'Col',  'Capt', 'Dona'], 'Rare')\n",
    "test['Title'] = test['Title'].replace(['Ms'], 'Miss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop name column as title has been extracted\n",
    "\n",
    "\n",
    "train = train.drop('Name', axis = 1)\n",
    "train.head()\n",
    "\n",
    "test = test.drop('Name', axis = 1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 IsAlone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "train[['SibSp', 'Parch', 'FamilySize']].head()\n",
    "\n",
    "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
    "test[['SibSp', 'Parch', 'FamilySize']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create IsAlone feature as familySize may have more information than we need, leading to overfitting\n",
    "\n",
    "train['IsAlone'] = 0\n",
    "train.loc[train['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "test['IsAlone'] = 0\n",
    "test.loc[test['FamilySize'] == 1, 'IsAlone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop SibSp, Parch and FamilySize as this is contained in isAlone\n",
    "\n",
    "train = train.drop(['SibSp', 'Parch','FamilySize'], axis = 1)\n",
    "test = test.drop(['SibSp', 'Parch','FamilySize'], axis = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.3 Age*Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convert Age into an ordinal variable. Group Ages into 5 age bands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train['AgeBand'] = pd.cut(train['Age'], 5)\n",
    "test['AgeBand'] = pd.cut(test['Age'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train['Age'] <= 16.136, 'Age'] = 0\n",
    "train.loc[(train['Age'] > 16.136) & (train['Age'] <= 32.102), 'Age'] = 1\n",
    "train.loc[(train['Age'] > 32.102) & (train['Age'] <= 48.068), 'Age'] = 2\n",
    "train.loc[(train['Age'] > 48.068) & (train['Age'] <= 64.034), 'Age'] = 3\n",
    "train.loc[train['Age'] > 64.034 , 'Age'] = 4\n",
    "\n",
    "test.loc[test['Age'] <= 16.136, 'Age'] = 0\n",
    "test.loc[(test['Age'] > 16.136) & (test['Age'] <= 32.102), 'Age'] = 1\n",
    "test.loc[(test['Age'] > 32.102) & (test['Age'] <= 48.068), 'Age'] = 2\n",
    "test.loc[(test['Age'] > 48.068) & (test['Age'] <= 64.034), 'Age'] = 3\n",
    "test.loc[test['Age'] > 64.034 , 'Age'] = 4\n",
    "\n",
    "# Drop age band feature\n",
    "train = train.drop('AgeBand', axis = 1)\n",
    "test = test.drop('AgeBand', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert ordinal Age into integer\n",
    "train['Age'] = train['Age'].astype('int')\n",
    "test['Age'] = test['Age'].astype('int')\n",
    "train['Age'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Age*Class\n",
    "\n",
    "train['Age*Class'] = train['Age'] * train['Pclass']\n",
    "test['Age*Class'] = test['Age'] * test['Pclass']\n",
    "train[['Age', 'Pclass', 'Age*Class']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin Fare \n",
    "train['FareBand'] = pd.qcut(train['Fare'], 4)\n",
    "test['FareBand'] = pd.qcut(test['Fare'], 4)\n",
    "train['FareBand'].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordinal encoding, simliar to age\n",
    "train.loc[train['Fare'] <= 2.066, 'Fare'] = 0\n",
    "train.loc[(train['Fare'] > 2.066) & (train['Fare'] <= 2.671), 'Fare'] = 1\n",
    "train.loc[(train['Fare'] > 2.671) & (train['Fare'] <= 3.418), 'Fare'] = 2\n",
    "train.loc[train['Fare'] > 3.418, 'Fare'] = 3\n",
    "\n",
    "test.loc[test['Fare'] <= 2.066, 'Fare'] = 0\n",
    "test.loc[(test['Fare'] > 2.066) & (test['Fare'] <= 2.671), 'Fare'] = 1\n",
    "test.loc[(test['Fare'] > 2.671) & (test['Fare'] <= 3.418), 'Fare'] = 2\n",
    "test.loc[test['Fare'] > 3.418, 'Fare'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop([ 'FareBand'], axis = 1)\n",
    "test = test.drop(['FareBand'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Fare into integer\n",
    "train['Fare'] = train['Fare'].astype('int')\n",
    "test['Fare'] = test['Fare'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Feature encoding \n",
    "\n",
    "Variables must be numeric to use for machine learning. Age and Fare were done when Binning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "label = LabelEncoder() \n",
    "train['Embarked'] = label.fit_transform(train['Embarked'])\n",
    "test['Embarked'] = label.fit_transform(test['Embarked'])\n",
    "train['Title'] = label.fit_transform(train['Title'])\n",
    "test['Title'] = label.fit_transform(test['Title'])\n",
    "train['Sex'] = train['Sex'].map({'male': 0, 'female': 1})\n",
    "test['Sex'] = test['Sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train.drop('PassengerId', axis = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Survived'] = train['Survived'].astype('int')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelling\n",
    "\n",
    "Scikit-learn is one of the most popular libraries for machine learning in Python and that is what we will use in the modelling part of this project. \n",
    "\n",
    "Since Titanic is a classfication problem, we will need to use classfication models, also known as classifiers, to train on our model to make predictions. I highly recommend checking out this scikit-learn [documentation](https://scikit-learn.org/stable/index.html) for more information on the different machine learning models available in their library. I have chosen the following classifiers for the job:\n",
    "\n",
    "- Logistic regression\n",
    "- Support vector machines\n",
    "- K-nearest neighbours\n",
    "- Gaussian naive bayes\n",
    "- Perceptron\n",
    "- Linear SVC\n",
    "- Stochastic gradient descent\n",
    "- Decision tree\n",
    "- Random forest\n",
    "- CatBoost\n",
    "\n",
    "In this section of the notebook, I will fit the models to the training set as outlined above and evaluate their accuracy at making predictions. Once the best model is determined, I will also do hyperparameter tuning to further boost the performance of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Split training data\n",
    "\n",
    "We need to first split our training data into independent variables or predictor variables, represented by X as well as  dependent variable or response variable, represented by Y.\n",
    "\n",
    "Y_train is the survived column in our training set and X_train is the other columns in the training set excluding the Survived column. Our models will learn to classify survival, Y_train based on all X_train and make predictions on X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('Survived', axis = 1)\n",
    "Y_train = train['Survived']\n",
    "X_test = test.drop('PassengerId', axis = 1).copy()#why only drop now\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Fit model to data and make predictions\n",
    "\n",
    "This requires 3 simple steps: instantiate the model, fit the model to the training set and predict the data in test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Explanation (not to be included in final submision): In section 6.2, we are training our models using the ENTIRE training set (every row that has a survive column). The models are UNTUNED.. We then calculate the accuracy of each model for the TRAINING set data. In other words we  determine how accurate each model is when it is asked to predict the outcome  (survival)  for the passengers on which it was trained. High scores might be an inidcation of which algorithms are likely to work well for predicting survival for passenges in the test set(this is the ultimate goal), although high scores could also indicate overfitting which is bad . These scores are summarised in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "logreg = LogisticRegression()\n",
    "LGtrained=logreg.fit(X_train, Y_train)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Support vector machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 K-nearest neighbours (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "KNNtrained=knn.fit(X_train, Y_train)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Gaussian naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.6 Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.7 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.8 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.9 Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "random_forest = RandomForestClassifier(n_estimators = 100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.10 CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "catboost = CatBoostClassifier()\n",
    "catboost.fit(X_train, Y_train)\n",
    "acc_catboost = round(catboost.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "#MLP\n",
    "mlp = MLPClassifier()\n",
    "mlp.fit(X_train, Y_train)\n",
    "acc_mlp = round(catboost.score(X_train, Y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rael\n",
    "#acc_catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Model evaluation and hyperparameter tuning\n",
    "\n",
    "Once all our models have been trained, the next step is to assess the performance of these models and select the one which has the highest prediction accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Training accuracy\n",
    "\n",
    "Training accuracy shows how well our model has learned from the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internal comment: Viewing and summarising the scores calcualted above for each algorithm. These models have have not yet been tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "                                 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', \n",
    "                                 'Linear SVC', 'Decision Tree', 'CatBoost','MLP'],\n",
    "                       'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron,\n",
    "                                 acc_sgd, acc_linear_svc, acc_decision_tree, acc_catboost, acc_mlp]})\n",
    "\n",
    "models.sort_values(by = 'Score', ascending = False, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 K-fold cross validation\n",
    "\n",
    "It is important to not get too carried away with models with impressive training accuracy as what we should focus on instead is the model's ability to predict out-of-samples data, in other words, data our model has not seen before.\n",
    "\n",
    "This is where k-fold cross validation comes in. K-fold cross validation is a technique whereby a subset of our training set is kept aside and will act as holdout set for testing purposes. Here is a great [video](https://www.youtube.com/watch?v=fSytzGwwBVw) explaining the concept in more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list which contains classifiers \n",
    "\n",
    "classifiers = []\n",
    "classifiers.append(LogisticRegression())\n",
    "classifiers.append(SVC())\n",
    "classifiers.append(KNeighborsClassifier(n_neighbors = 5))\n",
    "classifiers.append(GaussianNB())\n",
    "classifiers.append(Perceptron())\n",
    "classifiers.append(LinearSVC())\n",
    "classifiers.append(SGDClassifier())\n",
    "classifiers.append(DecisionTreeClassifier())\n",
    "classifiers.append(RandomForestClassifier())\n",
    "classifiers.append(CatBoostClassifier())\n",
    "classifiers.append(MLPClassifier())\n",
    "\n",
    "\n",
    "len(classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list which contains cross validation results for each classifier\n",
    "\n",
    "cv_results = []\n",
    "for classifier in classifiers:#each result has 10 subcompoents for each section of the data that was made test if cv equals 10\n",
    "    cv_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy', cv = 5))#try other cv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation of cross validation results for each classifier  \n",
    "\n",
    "cv_mean = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_mean.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame({'Cross Validation Mean': cv_mean, 'Cross Validation Std': cv_std, 'Algorithm': ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Gausian Naive Bayes', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest', 'CatBoost','MLP']})\n",
    "cv_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('Cross Validation Mean', 'Algorithm', data = cv_res, order = cv_res.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cv_std})\n",
    "plt.ylabel('Algorithm')\n",
    "plt.title('Cross Validation Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, support vector machines has the highest cross validation mean and thus we will proceed with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 Hyperparameter tuning for SVM\n",
    "\n",
    "Hyperparameter tuning is the process of tuning the parameters of a model. Here I will tune the parameters of support vector classifier using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_grid = {'n_neighbors': [1,2,3,4,5,6],  \n",
    "#              # 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "#              # 'kernel': ['rbf']}  \n",
    "# }\n",
    "# grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose = 3) \n",
    "\n",
    "# grid.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'alpha': [0,1e-5,1e-4,1e-3,1e-2,1e-1,1],  \n",
    "             # 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "             # 'kernel': ['rbf']}  \n",
    "}\n",
    "grid = GridSearchCV(MLPClassifier(), param_grid, refit = True, verbose = 3) \n",
    "\n",
    "grid.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best parameters: \", grid.best_params_) \n",
    "print(\"Best estimator: \", grid.best_estimator_)# what is this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy\n",
    "\n",
    "# #svc = KNeighborsClassifier(C = 100, gamma = 0.01, kernel = 'rbf')\n",
    "# knn = KNeighborsClassifier(n_neighbors=6)\n",
    "# knn.fit(X_train,Y_train)\n",
    "# #svc.fit(X_train, Y_train)\n",
    "# Y_pred = knn.predict(X_test)\n",
    "# acc_svc = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "# acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svc = KNeighborsClassifier(C = 100, gamma = 0.01, kernel = 'rbf')\n",
    "mlp = grid.best_estimator_\n",
    "trainedAndTunedlMLP=mlp.fit(X_train,Y_train)\n",
    "#svc.fit(X_train, Y_train)\n",
    "Y_pred = mlp.predict(X_test)\n",
    "acc_svc = round(mlp.score(X_train, Y_train) * 100, 2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mean cross validation score\n",
    "\n",
    "cross_val_score(mlp, X_train, Y_train, scoring = 'accuracy', cv = 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mean cross validation score improved slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival predictions by support vector classifier\n",
    "\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trained_MLP =trainedAndTunedlMLP #still not sure what this estimator is\n",
    "trained_knn = KNNtrained\n",
    "trained_lg = LGtrained\n",
    "\n",
    "\n",
    "voting_clf_soft = VotingClassifier(estimators = [('mlp',best_trained_MLP),('knn',trained_knn),('lg',trained_lg)], voting = 'soft') \n",
    "\n",
    "\n",
    "\n",
    "print('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,Y_train,cv=5))\n",
    "print('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,Y_train,cv=5).mean())\n",
    "\n",
    "params = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n",
    "\n",
    "vote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\n",
    "best_clf_weight = vote_weight.fit(X_train, Y_train)\n",
    "voting_clf_sub = best_clf_weight.best_estimator_.predict(X_test)\n",
    "\n",
    "voting_clf_soft.fit(X_train, Y_train)\n",
    "Y_pred =  voting_clf_soft.predict(X_test).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Preparing data for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our submission dataframe to have 418 rows and 2 columns, PassengerId and Survived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "\n",
    "submit = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe is ready for submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save csv file \n",
    "\n",
    "submit.to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Possible extensions to improve model accuracy\n",
    "\n",
    "1. Analyse ticket and cabin features\n",
    "    - Do these features help predict passenger survival?\n",
    "    - If yes, consider including them in the training set instead of dropping\n",
    "2. Come up with alternative features in feature engineering\n",
    "    - Is there any other features you can potentially create from existing features in the dataset\n",
    "3. Remove features that are less important\n",
    "    - Does removing features help reduce overfitting in the model?\n",
    "4. Ensemble modelling\n",
    "    - This is a more advanced technique whereby you combine prediction results from multiple machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Conclusion\n",
    "\n",
    "You should achieve a submission score of 0.77511 if you follow exactly what I have done in this notebook. In other words, I have correctly predicted 77.5% of the test set. I highly encourage you to work through this project again and see if you can improve on this result.\n",
    "\n",
    "If you found any mistakes in the notebook or places where I can potentially improve on, feel free to reach out to me. Let's help each other get better - happy learning!\n",
    "\n",
    "My platforms: \n",
    "- [Facebook](https://www.facebook.com/chongjason914)\n",
    "- [Instagram](https://www.instagram.com/chongjason914)\n",
    "- [Twitter](https://www.twitter.com/chongjason914)\n",
    "- [LinkedIn](https://www.linkedin.com/in/chongjason914)\n",
    "- [YouTube](https://www.youtube.com/channel/UCQXiCnjatxiAKgWjoUlM-Xg?view_as=subscriber)\n",
    "- [Medium](https://www.medium.com/@chongjason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "https://github.com/chongjason914/kaggle-titanic \n",
    "\n",
    "https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n",
    "\n",
    "https://github.com/murilogustineli/Titanic-Classification\n",
    "\n",
    "https://www.kaggle.com/code/kenjee/titanic-project-example/notebook\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c36ee19e8b3149dddcbe35ab5f92ddf427ca8ae53f8aa17469b1495a8f0d9a75"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
